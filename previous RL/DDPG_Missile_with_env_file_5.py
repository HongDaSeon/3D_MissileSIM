#!/usr/bin/env python
# coding: utf-8
################################################################################################################################################################################ -ohh`  #####################                
#                                                                                                                                                                             -ohNMMMMd`                    #
#                                                                                                                                                                         -ohNMMMMMmMMMd`                   #
#                                                                                                                                                                     -ohNMMMMNho-  yMMMd`                  #
#                                                                                                                                                                 -ohNMMMMNho-       yMMMd`                 #
#                                                                                                                                                             -ohNMMMMNho-            yMMMd`                #
#                                                                                                                                                         -+hNMMMMMdo-                 yMMMd`               #
#                                                                                                                                                     -+hNMMMMMdo:`                     yMMMd`              #
#                                                                                                                                                 -+hNMMMMMho-                           sMMMd`             #
#                                                                                                                                                `dMMMMho-                                sMMMd`            #
#                                                                                                                                                 `dMMMo                                   sMMMd`           #
#                                                                                                                                                  `dMMMs                                   sMMMd`          #
#                                                                                                                                                   `dMMMs                                   sMMMm`         #
#                                                                                                                                                    `dMMMo         `:osyyo/.                 sMMMm`        #
#                                                                                                                                                     `dMMMo      `sNMMMMMMMMh-                sMMMm`       #
#    MMMMMMMMo mMMMMNNds-    -yNMMMMNd-      sMMs                -MMh                                    `ddh                                          `dMMMo    `mMMMMMMMMMMMM/                sMMMm`      #
#    MMM/----. mMMo--+mMMy  oMMN+.`./h:      sMMy      -+osso/.  -MMh-+so:    :osso/`  .++:`/+`-+osso/. `+MMN++/  -+sso/.  `++/`:+/++-   :++.           `dMMMo   oMMMMMMMMMMMMMm                 sMMMm.     #
#    MMMyssss` mMM/   `mMM/.MMM-             sMMy      oysosmMM/ -MMNhydMMy -mMNsodMN/ /MMNMNN.oysosmMM/.yMMMyyo`dMMyohMMo .MMNNNN+NMN. -MMd             `dMMMs  oMMMMMMMMMMMMMm              ./ymMMMMm`    #
#    MMMhyyyy` mMM/    dMMo-MMM`             sMMy      .+syhmMMs -MMh   NMM.hMM/  `MMM`/MMh    .+syhmMMs `MMN   sMMo   mMM-.MMm`   :MMh dMN.              `dMMMs `mMMMMMMMMMMMM:          `/ymMMMMMms/`     #
#    MMM.      mMM/  `oMMm` mMMy`    /.      sMMy     :MMd:-oMMs -MMh  `NMM`hMM/  `MMN`/MMy   :MMd:-oMMs `MMN   oMMs   mMM-.MMm     oMMdMM:                `dMMMs `sNMMMMMMMMh-       `/smMMMMMms/`         #
#    MMM.      mMMmdmMMNs`  `yMMNdhdNM:      sMMNmmmmd-MMNssmMMs -MMNyyNMN/ .mMNysmMN/ /MMy   -MMNssmMMs  mMMhss`hMMysdMMo .MMm      hMMMs                  `dMMMs   -+ssso/`     `/smMMMMMms/`             #
#    ///`      //////:.       `:+oo+:.       -//////// .+o+:.//- `//::++/`    -+oo+:`  .//-    .+o+:.//-   :+o+:  ./ooo/`  `///      +MMd                    `dMMMs           ./smMMMMMms/`                 #
#                                                                                                                                   .NMN.                     `dMMMs      ./ymMMMMMms/`                     #
#                                                                                                                                   `..`                       `hMMMs `/smMMMMMmy/`                         #
#                                                                                                                                                               `hMMMmMMMMMmy/.                             #
#                                             T  H  E    M  O  T  I  O  N    T  E  C  H  N  O  L  O  G  Y    I  N  N  O  V  A  T  I  O  N  S                     `hMMMMmy/.                                 #
#                                                                                                                                                                 `yy/.                                     #
#                                                                                                                                                                                                           #
#   ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  #
#                                                                                                                                                                                                           #
#                    `++++++/-`    `+++++++-   -+ooo+:  .++.     ./oooo+-   +++`     :+/   +++++++-  -++++++++++++//:.                                                                                      #
#                    .MMmyyhmMMh-  `MMmyyyy: -mMmsosyy  /MM:   +mMmyssydh   MMMN:    hMm   NMmyyyy/  oMMMMMMMMMMMMMMMMMmy+.                                                                                 #
#                    .MMo    -mMN. `MMo      sMM/       /MM:  hMN/          MMhNMo   hMm   NMy       oMMMMMMMMMMMMMMMMMMMMMh:                                                                               #
#                    .MMo     :MMo `MMmyyyy` `yMMmy+.   /MM: :MMo  `+++++.  MMs.dMh` hMm   NMmyyyy.  oMMMMM+     `.:odMMMMMMMy                                                                              #
#                    .MMo     /MM+ `MMh++++`   `:sdMMy  /MM: :MMo  `yydMM:  MMs  sMm-hMm   NMd++++`  oMMMMM+          .yMMMMMMy                                                                             #
#                    .MMo   `+NMh  `MMo      .     sMM: /MM:  dMN/    /MM:  MMs   /NMmMm   NMy       oMMMMM+            +MMMMMM:                                                                            #
#                    .MMNddmMMh/   `MMNdddds sNdyydMMy  /MM:  `omMNdhhNMM-  MMs    .dMMm   NMNddddy  oMMMMM+             hMMMMMy                                                                            #
#                     :::::-.       :::::::- `-////-`   `::`     .:///:-`   ::.      ::-   :::::::-  oMMMMM+             +MMMMMm                                                                            #
#                                                                                                    oMMMMM+             +MMMMMd                                                                            #
#                                                                                .-`                 oMMMMM+             hMMMMMs                                                                            #
#                                                                                dM:                 oMMMMM+            /MMMMMM. `mmm.    /hmmmm+ `mmmmmm`  /hmNNmy:   dmd.   -mh                           #
#                                                                                dM:-//- `//`  `/:   oMMMMM+          `oMMMMMM/  hMoMd   -MM.   ` `MM`    `dMs.  -hMy  mMmN/  :Md                           #
#                                                                                dMdo+hMy sMs  yM/   oMMMMM+       `:sNMMMMMm:  oMs yMo   sNNh+.  `MMysso +Mm     .MM. mM-sMs :Md                           #
#                                                                                dM:  `MM` dM::Ms    oMMMMMmhhhhddNMMMMMMMNo`  :MMssyMM:    -omMy `MM:::- /MN`    -MM` mM- :Nd/Md                           #
#                                                                                dMy..sMd  `mmNd     oMMMMMMMMMMMMMMMMMmy:    `NM+:::+MN`-+-.-sMm `MM/:::` yMd+::oNN/  mM-  .dMMd                           #
#                                                                                os/shy/    :MN.     /hhhhhhhhhhhyso+:`       /s+     +s/.syhys+` `ssssss-  .+yhys/`   os.    oso                           #
#                                                                                         /+mN-                                                                                                             #
######################################################################################## /+:` ###############################################################################################################
# Missile_Reinforcement_Learning_platform_DDPG


import vpython as vp
import Missile_ENV_V_3_Ldot_Vm as MissSim
import math as m
import csv
import sys
import numpy as np
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal
import torchvision.transforms as transforms
import time
import copy
import pickle
from collections import namedtuple
import matplotlib.pyplot as plt
import numpy as np
import random as rd



#basic initialization+++++++++++++++++++++++++++++++++++++++++++++
slept = False
animcounter = 0
aclr = 1e-7
crlr = 1e-6
# nn param
gpu_num = 1
mu_now = 0
hitCount = 0
saveCount = 0
solved = False
tau = 0.05


device = ('cuda'+':'+ str(gpu_num)) if torch.cuda.is_available() else 'cpu'

a_seed = 0
a_gamma = 0.9
max_step_count = 600

a_log_interval = 10
torch.manual_seed(a_seed)
np.random.seed(a_seed)
if device == ('cuda'+ ':' + str(gpu_num)) :
    torch.cuda.manual_seed_all(a_seed)

TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])
RewardRecord = namedtuple('RewardRecord', ['ep', 'reward'])
Transition = namedtuple('Transition', ['s', 'a', 'r', 's_'])

print(device)


class ActorNet(nn.Module):

    def __init__(self):
        super(ActorNet, self).__init__()
        self.fc     = nn.Linear(2, 400)
        self.hd     = nn.Linear(400, 600)
        self.hd2     = nn.Linear(600, 400)
        self.mu_layer = nn.Linear(400, 1)
        
        torch.nn.init.xavier_uniform_(self.fc.weight)
        torch.nn.init.xavier_uniform_(self.hd.weight)   
        torch.nn.init.xavier_uniform_(self.hd2.weight)  
        torch.nn.init.xavier_uniform_(self.mu_layer.weight)

    def forward(self, s):
        s   = s.to(device)
        x   = (self.fc(s))
        x   = F.tanh(self.hd(x))
        x   = F.tanh(self.hd2(x))
        acc = self.mu_layer(x) #2 .0 * F.tanh(self.mu_layer(x))
        acc = acc.to('cpu')
        return acc

class CriticNet(nn.Module):

    def __init__(self):
        super(CriticNet, self).__init__()
        self.fc = nn.Linear(3, 400)
        self.hd = nn.Linear(400, 600)
        self.hd2 = nn.Linear(600, 400)
        self.Q_layer = nn.Linear(400, 1)
        
        torch.nn.init.xavier_uniform_(self.fc.weight)
        torch.nn.init.xavier_uniform_(self.hd.weight)
        torch.nn.init.xavier_uniform_(self.hd2.weight)
        torch.nn.init.xavier_uniform_(self.Q_layer.weight)

    def forward(self, s, a):
        s   = s.to(device)
        a   = a.to(device)
        x = self.fc(torch.cat([s, a], dim=1))
        x = F.tanh(self.hd(x))
        x = F.tanh(self.hd2(x))
        state_value = self.Q_layer(x)
        state_value = state_value.to('cpu')
        return state_value


class Memory():

    memory_pointer = 0
    isfull = False

    def __init__(self, capacity):
        self.memory = np.empty(capacity, dtype=object)
        self.capacity = capacity

    def update(self, transition):
        self.memory[self.memory_pointer] = transition
        self.memory_pointer += 1
        if self.memory_pointer == self.capacity:
            self.memory_pointer = 0
            self.isfull = True

    def sample(self, batch_size):
        return np.random.choice(self.memory, batch_size)


class Agent():

    max_grad_norm = 0.5

    def __init__(self):
        self.training_step = 0
        self.var = 1
        self.eval_cnet, self.target_cnet = CriticNet().to(device).float(), CriticNet().to(device).float()
        self.eval_anet, self.target_anet = ActorNet().to(device).float(), ActorNet().to(device).float()
        self.memory = Memory(40000) #2000
        self.optimizer_c = optim.Adam(self.eval_cnet.parameters(), lr=crlr)
        self.optimizer_a = optim.Adam(self.eval_anet.parameters(), lr=aclr)

    def select_action(self, state):
        global mu_now
        state = torch.from_numpy(state).float().unsqueeze(0)
        mu = self.eval_anet(state)
        mu_now = copy.deepcopy(mu.item())
        dist = Normal(mu, torch.tensor(self.var, dtype=torch.float))
        action = dist.sample()
        action = action.clamp(-99, 99)
        return (action.item(),)

    def save_param(self, epsd):
        # pass
        torch.save(self.eval_anet.state_dict(), 'params/anet_params_R,initL,Ohm'+str(epsd)+'.pkl')
        torch.save(self.eval_cnet.state_dict(), 'params/cnet_params_R,initL,Ohm'+str(epsd)+'.pkl')

    def store_transition(self, transition):
        self.memory.update(transition)

    def update(self):
        self.training_step += 1

        transitions = self.memory.sample(200)
        s = torch.tensor([t.s for t in transitions], dtype=torch.float)
        a = torch.tensor([t.a for t in transitions], dtype=torch.float).view(-1, 1)
        r = torch.tensor([t.r for t in transitions], dtype=torch.float).view(-1, 1)
        s_ = torch.tensor([t.s_ for t in transitions], dtype=torch.float)

        with torch.no_grad():
            q_target = r + a_gamma * self.target_cnet(s_, self.target_anet(s_))
            
        q_eval = self.eval_cnet(s, a)

        # update critic net
        self.optimizer_c.zero_grad()
        c_loss = F.smooth_l1_loss(q_eval, q_target)
        with torch.no_grad():
            #lossset[1] = copy.deepcopy(c_loss.item())
            pass
        c_loss.backward()
        nn.utils.clip_grad_norm_(self.eval_cnet.parameters(), self.max_grad_norm)
        self.optimizer_c.step()

        # update actor net
        self.optimizer_a.zero_grad()
        a_loss = -self.eval_cnet(s, self.eval_anet(s)).mean()
        with torch.no_grad():
            #lossset[0] = copy.deepcopy(a_loss.item())
            pass
        a_loss.backward()
        nn.utils.clip_grad_norm_(self.eval_anet.parameters(), self.max_grad_norm)
        self.optimizer_a.step()

        for param, target_param in zip(self.eval_cnet.parameters(), self.target_cnet.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
        
        for param, target_param in zip(self.eval_anet.parameters(), self.target_anet.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

        self.var = max(self.var * 0.9999999, 0.01)

        return q_eval.mean().item()

def init_picker():
    radius_rand         = rd.random()*40000 + 10000 #rd.random()*20000+30000
    posang_rand         = rd.random()*m.pi*2
    x__pos_rand         = radius_rand*m.cos(posang_rand)
    y__pos_rand         = radius_rand*m.sin(posang_rand)
    tangen_rand         = abs(m.atan( (-y__pos_rand / -x__pos_rand) ))
    headin_seed         = m.pi/180*(90)*(rd.random()-0.5)

    Vm_rand             = rd.random()*400 + 200

    if x__pos_rand >=0:
        if y__pos_rand >=0:
            headin_rand = tangen_rand+m.pi
        else:
            headin_rand = m.pi-tangen_rand
    else:

        if y__pos_rand >=0:
            headin_rand = 2*m.pi-tangen_rand
        else:
            headin_rand = tangen_rand

    headin_rand = headin_rand+headin_seed

    if headin_rand < 0:
       headin_rand = headin_rand+2*m.pi
    
    print('\t'+'init'+str(x__pos_rand)+','+str(y__pos_rand)+','+str(Vm_rand))

    return x__pos_rand, y__pos_rand, headin_rand, Vm_rand


scene1 = vp.canvas(title = "close_look",
                x=0,y=0, width=500, height=300,
                range=10, background=vp.color.black,
                center = vp.vec(10,10,0))


g1 = vp.graph(title = "LOS",width = 500, height = 200)
g2 = vp.graph(title = "R", width = 500, height = 200)
g3 = vp.graph(title = "acc", width = 500, height = 200)
g4 = vp.graph(title = "reward", width = 500, height = 200)
g5 = vp.graph(title = "training_status", width = 500, height = 500)
g6 = vp.graph(title = "trajectory", width = 500, height = 500, xmin = -50000, xmax = 50000, ymin = -50000, ymax = 50000)

#Params
Vm = 250
initpos = [35000., 35000.]
initpsi = 0.
dt = 0.1 ##sec
animspeed = 400
animdt = dt / animspeed

def norm_R_reward(sacrifice):
    loged = -m.log10(-sacrifice)
    normV = loged/4
    return normV

def norm_A_reward(sacrifice,Vm,initLOS,t_f):
    stand = sacrifice/Vm/t_f/m.sqrt(abs(initLOS)+0.5)
    normV = ((-m.log(-stand))-3)/4
    return normV

a_render = True


# Generate Model Objects
missile_model_1 = MissSim.Missile_2D(Vm, initpos[0], initpos[1], initpsi, dt)
print(missile_model_1)
target_model_1  = MissSim.TargetShip(0,0,0,0)
print(target_model_1)
seeker_model_1  = MissSim.Seeker(missile_model_1, target_model_1)

# Plots initializtion
LOS_graph   = vp.gcurve(graph = g1, color = vp.color.cyan)
fLOS_graph  = vp.gcurve(graph = g1, color = vp.color.blue)
R_graph     = vp.gcurve(graph = g2, color = vp.color.magenta)
acc_graph   = vp.gcurve(graph = g3, color = vp.color.black)
mu_graph    = vp.gcurve(graph = g3, color = vp.color.red)
rew_graph   = vp.gcurve(graph = g4, color = vp.color.blue)
stt_graph   = vp.gcurve(graph = g5, color = vp.color.green)
trajectory  = vp.gdots(graph = g6, color = vp.vec(rd.random(),rd.random(),rd.random()), radius = 1)
missile_visual = vp.sphere(axis = missile_model_1.vizvel, size = vp.vec(2.01,0.5,0.5), color = vp.color.red,
                           accaxis = missile_model_1.accaxi, make_trail = True, retain = 600)
target_visual  = vp.box(length = 50, width = 15, height = 15, pos = target_model_1.vizpos)

missile_visual.pos = missile_model_1.vizpos
missile_visual.v   = missile_model_1.vizvel

#vp.attach_trail(missile_visual, retain = 600)

agent = Agent()

training_records    = []
Reward_records      = []

running_reward, running_q = -0.5, 0



for i_ep in range(100000):
    print('Episode : '+str(i_ep)+'------------------------')
    t = 0 ##sec
    Cassette_tape       = []
    score = 0
    xpos, ypos, hed, Vm = init_picker()
    missile_model_1.reset(xpos, ypos, hed, Vm, reset_flag = True)
    seeker_model_1.impactR = 50000
    seeker_model_1.RLdot = 1e-6
    _, initLOS, _, _, _, state = seeker_model_1.seek()
    closestR = 999999
    integral_acc = 0
    step_count = 0
    while t<max_step_count:
        #print(state)
        if a_render : vp.rate(1/animdt)
        action = agent.select_action(state)
        #print(action[0])
        missile_model_1.simulate(action)
        R_mez, _, _, _, _, state_ = seeker_model_1.seek()
        td_reward, mc_reward, done, is_hit = seeker_model_1.spit_reward(action)
        #if is_hit:
        #    reward = reward + 10000
        #state_, reward, done, _ = env.step(action)
        if t == max_step_count-1:
            done = True
        if done:
            pass
            #reward = reward + s_reward
        if is_hit:
            #reward = reward + 2
            hitCount += 1
            print('\t'+'hit!!!++++++++++++++++++++++!!!!!!!!!!!!!!!!!'+str(hitCount))
            if not solved : 
                pass
                #aclr = aclr/2
                #crlr = crlr/2
            #agent.optimizer_c = optim.Adam(agent.eval_cnet.parameters(), lr=crlr)
            #agent.optimizer_a = optim.Adam(agent.eval_anet.parameters(), lr=aclr)
        #score += td_reward
        if a_render & (not done):
            R, LOS, _, lpfLOS, _, _    = seeker_model_1.seek()
            LOS_graph.plot(t,LOS)
            fLOS_graph.plot(t,lpfLOS)
            R_graph.plot(t,R)
            acc_graph.plot(t,action[0])
            mu_graph.plot(t,mu_now)
            rew_graph.plot(t,td_reward)
            missile_visual.pos  = missile_model_1.vizpos
            missile_visual.axis = missile_model_1.vizvel
            if step_count==20:
                    trajectory.plot(missile_visual.pos.x, missile_visual.pos.y)
                    step_count = 0
            step_count +=1
            scene1.center = missile_visual.pos
        integral_acc += td_reward*dt
        Cassette_tape.append([state, action, state_])
        #agent.store_transition(Transition(state, action, (reward+25)/50 , state_))
        state = state_
        if closestR > R_mez:
            closestR = R_mez
        if agent.memory.isfull:
            q = agent.update()
            running_q = 0.99 * running_q + 0.01 * q
        t = t + dt
        if done:
            break
    #a_render = False

    #update transitions
    
    #final_reward = (mc_reward + integral_acc/t )
    final_reward = 0.7*norm_R_reward(mc_reward) + 0.3*norm_A_reward(integral_acc,Vm,initLOS,t)
    score = final_reward
    for rowrow in Cassette_tape:
        agent.store_transition(Transition(rowrow[0], rowrow[1], score, rowrow[2]))
    if not slept:
        running_reward = score
    slept = True
    seeker_model_1.filter1st = True
    print('\t'+'R:'+str(closestR)+', score:'+str(score))
    stt_graph.plot(i_ep,closestR)    
    if i_ep % 2 == 0:
        
        LOS_graph.delete()
        fLOS_graph.delete()
        R_graph.delete()
        acc_graph.delete()
        rew_graph.delete()
        mu_graph.delete()
        trajectory.delete()
        
    running_reward = running_reward * 0.9 + score * 0.1
    training_records.append(TrainingRecord(i_ep, running_reward))
    Reward_records.append(RewardRecord(i_ep, score))
    #print(i_ep, running_reward, running_q)
    if i_ep % a_log_interval == 0:
        print('Step {}\tAverage score: {:.2f}\tAverage Q: {:.2f}'.format(
            i_ep, running_reward, running_q))
        
        plt.plot([r.ep for r in Reward_records], [r.reward for r in Reward_records])
        plt.title('RWDs')
        plt.xlabel('Episode')
        plt.ylabel('reward sum')
        plt.savefig("img/reward.png")


        plt.plot([r.ep for r in training_records], [r.reward for r in training_records])
        plt.title('RWDs-LPF')
        plt.xlabel('Episode')
        plt.ylabel('reward sum')
        plt.savefig("img/reward_lpf.png")

        #a_render = True
    if is_hit:
        print("Solved! Running reward is now {}!".format(running_reward))
        #env.close()
        saveCount += 1
 
        agent.save_param(i_ep)

        if running_reward > 0: solved = True
        #with open('log/ddpg_training_records.pkl', 'wb') as f:
        #    pickle.dump(training_records, f)
        #break
    
#env.close()

plt.plot([r.ep for r in training_records], [r.reward for r in training_records])
plt.title('DDPG')
plt.xlabel('Episode')
plt.ylabel('Moving averaged episode reward')
plt.savefig("img/ddpg.png")
plt.show()
